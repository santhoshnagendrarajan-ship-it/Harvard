# Harvard Art Museum Artifact Explorer

A comprehensive data collection and analysis system that fetches, processes, and visualizes artifact data from the Harvard Art Museum API.

## ğŸ“‹ Overview

This project provides an interactive web application to explore Harvard Art Museum artifacts across multiple classifications. It fetches real-time data from the Harvard API, stores it in a MySQL database, and offers powerful querying and visualization capabilities.

## âœ¨ Features

- **Multi-Classification Support**: Explore artworks across 5 major categories:
  - Coins
  - Paintings
  - Sculptures
  - Jewellery
  - Drawings

- **Data Collection**: 
  - Fetches minimum 2,500 records per classification
  - Paginated API calls (25 pages Ã— 100 records)
  - Automatic data extraction and transformation

- **Database Management**:
  - Three normalized MySQL tables
  - Dynamic table creation and management
  - Data persistence and retrieval

- **Interactive UI**:
  - Web-based interface using Streamlit
  - Real-time data preview
  - SQL query execution and results visualization
  - One-click data insertion into database

- **Advanced Analytics**:
  - Pre-written SQL queries
  - Custom query support
  - Data visualization
  - Filtering and sorting capabilities

## ğŸ› ï¸ Tech Stack

- **Backend**: Python 3.x
- **Frontend**: Streamlit
- **Database**: MySQL
- **API Client**: Requests library
- **Data Processing**: Pandas
- **Database Driver**: mysql-connector-python

## ğŸ“¦ Installation

### Prerequisites
- Python 3.7+
- MySQL Server (running on localhost:3306)
- pip (Python package manager)

### Setup

1. **Clone or navigate to the project directory**:
   ```bash
   cd d:\GUVI\PROJECTS\Project1
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure MySQL Database**:
   - Start MySQL service
   - Create a database named `harvard`:
     ```sql
     CREATE DATABASE harvard;
     ```
   - Update credentials in `base_code.py` if needed (default: `root` user, password `1302`)

4. **Get API Key**:
   - Harvard Art Museums API key is embedded in `app.py`
   - To use your own key, get it from: https://www.harvardartmuseums.org/api

## ğŸš€ Usage

### Running the Application

```bash
streamlit run app.py
```

The application will open in your default browser at `http://localhost:8501`

### Workflow

1. **Select Classification**: Choose an artifact type from the dropdown
   
2. **Collect Data**: Click "Collect Data" button
   - Fetches all records from Harvard API for selected classification
   - Extracts and structures the data
   - Ready for preview

3. **Preview Data**: 
   - "Show Data" - View raw classification data
   - "Show artifact_metadata Data" - View artifact details
   - "Show artifact_media Data" - View media information
   - "Show artifact_colors Data" - View color analysis

4. **Store in Database**: 
   - Click "Insert into SQL" to save data to MySQL
   - Automatically creates/recreates tables
   - Validates and inserts all three data types

5. **Query & Analyze**:
   - View stored table data with "Show [Table] Table" buttons
   - Select from pre-written queries in "Query & Visualization" section
   - Run queries and view results in real-time

## ğŸ“ Project Structure

```
Project1/
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ base_code.py                    # Core API & database functions
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ query.txt                       # Pre-written SQL queries
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ data.json                       # Sample/cached data
â”œâ”€â”€ columns                         # Column reference file
â”œâ”€â”€ tables.txt                      # Table structure reference
â”œâ”€â”€ queries.txt                     # Additional query documentation
â””â”€â”€ draftapp/                       # Backup/version files
    â”œâ”€â”€ base_code.py
    â”œâ”€â”€ draftapp.py
    â”œâ”€â”€ v1.0/
    â”‚   â”œâ”€â”€ app_v1.1.txt
    â”‚   â””â”€â”€ base_code_v1.1.txt
    â””â”€â”€ basecodev1.txt
```

## ğŸ—„ï¸ Database Schema

### artifact_metadata
Stores core artifact information:
- `id` (PRIMARY KEY)
- `title`, `culture`, `period`, `century`
- `medium`, `dimensions`, `description`
- `department`, `classification`
- `accessionyear`, `accessionmethod`

### artifact_media
Stores media-related information:
- `objectid` (FK to artifact_metadata.id)
- `imagecount`, `mediacount`, `colorcount`
- `rank`, `datebegin`, `dateend`

### artifact_colors
Stores color analysis data:
- `objectid` (FK to artifact_metadata.id)
- `color`, `spectrum`, `hue`
- `percent`, `css3`

## ğŸ”‘ Key Functions

### Data Fetching
- `fetch_classification_data(api_key)` - Fetch all classifications
- `fetch_segment_records(api_key, segment)` - Fetch records for specific classification
- `get_segment_record(data, segment_name)` - Find segment by name

### Data Processing
- `build_artifact_metadata(data)` - Extract metadata
- `build_artifact_media(data)` - Extract media information
- `build_artifact_colors(data)` - Extract color data

### Database Operations
- `connect_mysql()` - Establish database connection
- `drop_all_tables(cursor)` - Drop existing tables
- `recreate_artifact_*_table(cursor)` - Create tables
- `insert_artifact_*(data, cursor, connection)` - Insert data into tables

## âš™ï¸ Configuration

### MySQL Connection
Edit connection parameters in `base_code.py`:
```python
connection = c.connect(
    host='127.0.0.1',
    user='root',
    password='1302',
    database='harvard',
    port=3306
)
```

### API Key
Located in `app.py`:
```python
api_key = "81ecd2aa-fab3-4f24-8503-67ef2e86d595"
```

## ğŸ“Š Querying

Pre-written queries are stored in `query.txt` and include analysis such as:
- Artifact counts by classification
- Color analysis and frequency
- Temporal distribution
- Cultural and period-based analysis
- Custom aggregations

## âš ï¸ Important Notes

- **API Rate Limits**: The Harvard API has rate limiting. Adjust pagination if needed.
- **Database Reset**: Clicking "Insert into SQL" drops and recreates all tables. Existing data will be lost.
- **Memory**: Loading 2,500+ records may require processing time. Be patient with large classifications.
- **Network**: Requires internet connection to fetch from Harvard API.

## ğŸ› Troubleshooting

**MySQL Connection Error**:
- Ensure MySQL server is running
- Check credentials in `base_code.py`
- Verify database `harvard` exists

**API Connection Error**:
- Check internet connection
- Verify API key is valid
- Check Harvard API status

**Data Not Displaying**:
- Click "Collect Data" before viewing
- Ensure data was successfully fetched (check console for errors)
- Verify MySQL connection is active

## ğŸ“ Dependencies

See `requirements.txt`:
```
streamlit
pandas
mysql-connector-python
requests
```

## ğŸ”„ Workflow Diagram

```
API (Harvard Art Museums)
        â†“
fetch_segment_records()
        â†“
build_artifact_* â†’ Streamlit UI
        â†“
insert_artifact_* â†’ MySQL
        â†“
SQL Queries â†’ Visualization
```

## ğŸ“š References

- [Harvard Art Museums API](https://www.harvardartmuseums.org/api)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [MySQL Documentation](https://dev.mysql.com/doc/)

## ğŸ‘¤ Author Notes

This project demonstrates:
- RESTful API integration
- Data transformation and normalization
- Database design and management
- Interactive web application development
- SQL query optimization
- Python best practices

---

**Last Updated**: February 22, 2026
